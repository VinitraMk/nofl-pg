{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models.autoencoder as vae\n",
    "import models.nfl as nfl\n",
    "import models.credence as credence\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from common.utils import sample_hyperparameter\n",
    "from common.evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for showing probability distributions\n",
    "\n",
    "def view_distribution(gt_df, generated_df, colnames, figsize = (15,30)):\n",
    "\n",
    "    # print(X[:,0].shape, generated_df[1][\"X1\"].shape)\n",
    "    # print('true value range', X[:, 0].min(), X[:, 0].max())\n",
    "    # print('gen value range', generated_df[0][\"X1\"].min(), generated_df[0][\"X1\"].max())\n",
    "\n",
    "    #plt.show()\n",
    "    nrows = len(colnames)\n",
    "    size = figsize\n",
    "    fig,ax = plt.subplots(nrows=nrows,ncols=2,figsize=size)\n",
    "\n",
    "    for i,yvar in enumerate(colnames):\n",
    "        sns.kdeplot(gt_df[yvar], ax = ax[i,0], fill = True)\n",
    "        ax[i,0].set_title(f'Observed {yvar}')\n",
    "        sns.kdeplot(generated_df[yvar], ax = ax[i,1], fill = True)\n",
    "        ax[i,1].set_title(f'Generated {yvar}')\n",
    "\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct experiment\n",
    "\n",
    "def use_nfl(gt_df, x_vars, y_vars, out_vars, treat_vars, categorical_vars, num_vars, sample_params, max_epochs):\n",
    "    nfl_obj = nfl.NFL(\n",
    "        data = gt_df,\n",
    "        outcome_var = out_vars,\n",
    "        treatment_var = treat_vars,\n",
    "        categorical_var = categorical_vars,\n",
    "        numerical_var=x_vars+num_vars\n",
    "    )\n",
    "    #sample_params = sample_hyperparameter(['kld_rigidity'], [(0.0,1.0)])\n",
    "    #max_epochs = 5 \n",
    "    #print('\\n\\nHyperparameters')\n",
    "    #print('kld rigidity:', sample_params['kld_rigidity'])\n",
    "    #print('max epochs', max_epochs, '\\n\\n')\n",
    "    gen_models = nfl_obj.fit(latent_dim = 4, hidden_dim = [8,16,8], kld_rigidity = sample_params['kld_rigidity'], max_epochs = max_epochs)\n",
    "\n",
    "    # generated samples\n",
    "    generated_df, generated_df_prime = nfl_obj.sample()\n",
    "    generated_df_prime['Y'] = (generated_df_prime['A'] * generated_df_prime['Y1']) + ((1 - generated_df_prime['A']) * generated_df_prime['Y0'])\n",
    "    generated_df_prime['Y_cf'] = (generated_df_prime['A'] * generated_df_prime['Yprime1']) + ((1 - generated_df_prime['A']) * generated_df_prime['Yprime0'])\n",
    "    view_distribution(gt_df, generated_df_prime, y_vars, (10,10))\n",
    "    #print(generated_df_prime['Y'].shape, gt_df['Y'].shape, generated_df_prime['Y_cf'].shape, gt_df['Y_cf'].shape)\n",
    "    fids = fid_score(gt_df, generated_df_prime, y_vars)\n",
    "    iss = inception_score(generated_df_prime, y_vars)\n",
    "    print('FID score: ', fids)\n",
    "    print('Inception score: ', iss)\n",
    "\n",
    "\n",
    "def use_credence(gt_df, x_vars, y_vars, out_vars, treat_vars, categorical_vars, num_vars, sample_params, max_epochs):\n",
    "    cred_obj = credence.Credence(\n",
    "        data = gt_df,\n",
    "        outcome_var = out_vars,\n",
    "        treatment_var = treat_vars,\n",
    "        categorical_var = categorical_vars,\n",
    "        numerical_var=x_vars+num_vars\n",
    "    )\n",
    "    #sample_params = sample_hyperparameter(['kld_rigidity'], [(0.0,1.0)])\n",
    "    #max_epochs = 5\n",
    "    #print('\\n\\nHyperparameters')\n",
    "    #print('kld rigidity:', sample_params['kld_rigidity'])\n",
    "    #print('max epochs', max_epochs, '\\n\\n')\n",
    "    gen_models = cred_obj.fit(latent_dim = 4, hidden_dim = [8,16,8], kld_rigidity = sample_params['kld_rigidity'], max_epochs = max_epochs)\n",
    "\n",
    "    # generated samples\n",
    "    generated_df, generated_df_prime = cred_obj.sample()\n",
    "    generated_df_prime['Y'] = (generated_df_prime['A'] * generated_df_prime['Y1']) + ((1 - generated_df_prime['A']) * generated_df_prime['Y0'])\n",
    "    generated_df_prime['Y_cf'] = (generated_df_prime['A'] * generated_df_prime['Yprime1']) + ((1 - generated_df_prime['A']) * generated_df_prime['Yprime0'])\n",
    "    view_distribution(gt_df, generated_df_prime, x_vars, (25,50))\n",
    "    view_distribution(gt_df, generated_df_prime, y_vars, (10,10))\n",
    "    #print(generated_df_prime['Y'].shape, gt_df['Y'].shape)\n",
    "    fids = fid_score(gt_df, generated_df_prime, y_vars)\n",
    "    iss = inception_score(generated_df_prime, y_vars)\n",
    "    print('FID score: ', fids)\n",
    "    print('Inception score: ', iss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eab5e5cf9d401290d89f1b91745f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', options=('acic19_linear', 'acic19_polynomial', 'toy'), value='acic19_linear')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d74b42504045268a3af4c6bcd7089d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Framework: ', options=('nfl', 'credence'), value='nfl')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define datasets\n",
    "#w = widgets.IntSlider()\n",
    "datasets = ['acic19_linear', 'acic19_polynomial', 'toy']\n",
    "wd = widgets.Dropdown(\n",
    "    options=datasets,\n",
    "    value='acic19_linear',\n",
    "    description='Dataset:',\n",
    ")\n",
    "frameworks = ['nfl', 'credence']\n",
    "wf = widgets.Dropdown(\n",
    "    options = frameworks,\n",
    "    value = 'nfl', \n",
    "    description = 'Framework: '\n",
    ")\n",
    "display(wd)\n",
    "display(wf)\n",
    "dataset_type = wd.value\n",
    "framework_type = wf.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "{'kld_rigidity': 0.13313563266863063}\n",
      "Max epochs:  250\n"
     ]
    }
   ],
   "source": [
    "sample_params = sample_hyperparameter(['kld_rigidity'], [(0.0,0.3)])\n",
    "max_epochs = 250 \n",
    "\n",
    "print('Hyperparameters')\n",
    "print(sample_params)\n",
    "print('Max epochs: ', max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.co ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataset to be used:  acic19_linear\n",
      "Framework to be used:  nfl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /work/pi_jensen_umass_edu/vmuralikrish_umass_edu/playground/nofl-pg/lightning_logs/version_25486798/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential | 296    | train\n",
      "1 | en_mu     | Linear     | 36     | train\n",
      "2 | en_logvar | Linear     | 36     | train\n",
      "3 | decoder   | Sequential | 400    | train\n",
      "4 | decode_Y  | Linear     | 9      | train\n",
      "-------------------------------------------------\n",
      "777       Trainable params\n",
      "0         Non-trainable params\n",
      "777       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=250` reached.\n",
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.co ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /work/pi_jensen_umass_edu/vmuralikrish_umass_edu/playground/nofl-pg/lightning_logs/version_25486798/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential | 296    | train\n",
      "1 | en_mu     | Linear     | 36     | train\n",
      "2 | en_logvar | Linear     | 36     | train\n",
      "3 | decoder   | Sequential | 408    | train\n",
      "4 | decode_Y  | Linear     | 18     | train\n",
      "-------------------------------------------------\n",
      "794       Trainable params\n",
      "0         Non-trainable params\n",
      "794       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/work/pi_jensen_umass_edu/vmuralikrish_umass_edu/.conda/envs/mlprojects/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nDataset to be used: ', wd.value)\n",
    "print('Framework to be used: ', wf.value)\n",
    "\n",
    "if wd.value == 'toy':\n",
    "    # generating toy dataset\n",
    "    X = np.random.normal(0, 1, (2000, 5))\n",
    "    Y0 = np.random.normal(np.sum(X,axis=1),1)\n",
    "    T = np.random.binomial(1,0.5,size=(X.shape[0],))\n",
    "    Y1 = Y0**2 + np.random.normal(np.mean(X,axis=1),5)\n",
    "    Y = T*Y1 + (1 - T)*Y0\n",
    "    xnames = ['X%d'%(i) for i in range(X.shape[1])]\n",
    "    ynames = ['Y', 'Y_cf']\n",
    "\n",
    "    gt_df = pd.DataFrame(X, columns=['X%d'%(i) for i in range(X.shape[1])])\n",
    "    gt_df['Y'] = T*Y1 + (1 - T)*Y0\n",
    "    gt_df['T'] = T\n",
    "\n",
    "    if wf.value == 'nfl':\n",
    "        use_nfl(gt_df, xnames, ynames, ['Y'], ['T'], ['T'], ['Y'], sample_params, max_epochs)\n",
    "    elif wf.value == 'credence':\n",
    "        use_credence(gt_df, xnames, ynames, ['Y'], ['T'], ['T'], ['Y'], sample_params, max_epochs)\n",
    "    else:\n",
    "        SystemExit('Invalid framework type provided!')\n",
    "elif wd.value == 'acic19_linear':\n",
    "    gt_df = pd.read_csv('./data/datasets/acic19_low_dim_1_linear.csv')\n",
    "\n",
    "    x_vars = ['V%d'%(i) for i in range(1,11)]\n",
    "    y_vars = ['Y', 'Y_cf']\n",
    "\n",
    "    if wf.value == 'nfl':\n",
    "        use_nfl(gt_df, x_vars, y_vars, ['Y'], ['A'], ['A'], ['Y'], sample_params, max_epochs)\n",
    "    elif wf.value == 'credence':\n",
    "        use_credence(gt_df, x_vars, y_vars, ['Y'], ['A'], ['A'], ['Y'], sample_params, max_epochs)\n",
    "    else:\n",
    "        SystemExit('Invalid framework type provided!')\n",
    "elif wd.value == 'acic19_polynomial':\n",
    "    gt_df = pd.read_csv('./data/datasets/acic19_low_dim_1_polynomial.csv')\n",
    "\n",
    "    x_vars = ['V%d'%(i) for i in range(1,11)]\n",
    "    y_vars = ['Y', 'Y_cf']\n",
    "\n",
    "    if wf.value == 'nfl':\n",
    "        use_nfl(gt_df, x_vars, y_vars, ['Y'], ['A'], ['A'], ['Y'], sample_params, max_epochs)\n",
    "    elif wf.value == 'credence':\n",
    "        use_credence(gt_df, x_vars, y_vars, ['Y'], ['A'], ['A'], ['Y'], sample_params, max_epochs)\n",
    "    else:\n",
    "        SystemExit('Invalid framework type provided!')\n",
    "else:\n",
    "    SystemExit('Invalid dataset value provided!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
